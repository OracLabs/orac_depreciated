{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi drips example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peaking at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `simulate.py` will insert data into two topics. The first topic contains all the taxi departures. The topic is located in the default Redpanda broker provided by Beaver. Beaver also provides a default Materialize instance to process the data in Redpanda with SQL. We'll do just that to take a look at the taxi departures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "\n",
    "conn = psycopg.connect(\"postgresql://materialize@localhost:6875/materialize?sslmode=disable\")\n",
    "conn.autocommit = True\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"DROP VIEW IF EXISTS taxi_departures\")\n",
    "    cur.execute(\"DROP SOURCE IF EXISTS taxi_departures_src\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE MATERIALIZED SOURCE taxi_departures_src\n",
    "    FROM KAFKA BROKER 'redpanda:29092' TOPIC 'taxi-departures'\n",
    "        KEY FORMAT TEXT\n",
    "        VALUE FORMAT BYTES\n",
    "        INCLUDE KEY AS trip_no, TIMESTAMP AS received_at;\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE VIEW taxi_departures AS (\n",
    "        SELECT\n",
    "            trip_no,\n",
    "            received_at,\n",
    "            CAST(CONVERT_FROM(data, 'utf8') AS JSONB) AS trip\n",
    "        FROM taxi_departures_src\n",
    "    );\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/qxhr4yq11msffjc0l_5279bc0000gn/T/ipykernel_38795/1265254364.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pd.read_sql('SELECT * FROM taxi_departures ORDER BY received_at DESC LIMIT 10', conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_no</th>\n",
       "      <th>received_at</th>\n",
       "      <th>trip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000009</td>\n",
       "      <td>2023-01-13 00:03:16.320</td>\n",
       "      <td>{'dropoff_latitude': 40.74546813964844, 'dropo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000008</td>\n",
       "      <td>2023-01-13 00:03:13.716</td>\n",
       "      <td>{'dropoff_latitude': 40.81161117553711, 'dropo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000007</td>\n",
       "      <td>2023-01-13 00:03:12.443</td>\n",
       "      <td>{'dropoff_latitude': 40.8165397644043, 'dropof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000006</td>\n",
       "      <td>2023-01-13 00:03:11.772</td>\n",
       "      <td>{'dropoff_latitude': 40.761734008789055, 'drop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000005</td>\n",
       "      <td>2023-01-13 00:03:11.502</td>\n",
       "      <td>{'dropoff_latitude': 40.84777069091797, 'dropo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0000004</td>\n",
       "      <td>2023-01-13 00:03:10.631</td>\n",
       "      <td>{'dropoff_latitude': 40.742988586425774, 'drop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0000003</td>\n",
       "      <td>2023-01-13 00:03:10.226</td>\n",
       "      <td>{'dropoff_latitude': 40.75033950805664, 'dropo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0000002</td>\n",
       "      <td>2023-01-13 00:03:09.355</td>\n",
       "      <td>{'dropoff_latitude': 40.81517028808594, 'dropo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0000001</td>\n",
       "      <td>2023-01-13 00:03:08.817</td>\n",
       "      <td>{'dropoff_latitude': 40.71749114990234, 'dropo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_no             received_at  \\\n",
       "0  0000009 2023-01-13 00:03:16.320   \n",
       "1  0000008 2023-01-13 00:03:13.716   \n",
       "2  0000007 2023-01-13 00:03:12.443   \n",
       "3  0000006 2023-01-13 00:03:11.772   \n",
       "4  0000005 2023-01-13 00:03:11.502   \n",
       "5  0000004 2023-01-13 00:03:10.631   \n",
       "6  0000003 2023-01-13 00:03:10.226   \n",
       "7  0000002 2023-01-13 00:03:09.355   \n",
       "8  0000001 2023-01-13 00:03:08.817   \n",
       "\n",
       "                                                trip  \n",
       "0  {'dropoff_latitude': 40.74546813964844, 'dropo...  \n",
       "1  {'dropoff_latitude': 40.81161117553711, 'dropo...  \n",
       "2  {'dropoff_latitude': 40.8165397644043, 'dropof...  \n",
       "3  {'dropoff_latitude': 40.761734008789055, 'drop...  \n",
       "4  {'dropoff_latitude': 40.84777069091797, 'dropo...  \n",
       "5  {'dropoff_latitude': 40.742988586425774, 'drop...  \n",
       "6  {'dropoff_latitude': 40.75033950805664, 'dropo...  \n",
       "7  {'dropoff_latitude': 40.81517028808594, 'dropo...  \n",
       "8  {'dropoff_latitude': 40.71749114990234, 'dropo...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_sql('SELECT * FROM taxi_departures ORDER BY received_at DESC LIMIT 10', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for taxi arrivals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/qxhr4yq11msffjc0l_5279bc0000gn/T/ipykernel_38795/3064716863.py:23: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pd.read_sql('SELECT * FROM taxi_arrivals ORDER BY received_at DESC LIMIT 10', conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_no</th>\n",
       "      <th>received_at</th>\n",
       "      <th>arrival</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [trip_no, received_at, arrival]\n",
       "Index: []"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"DROP VIEW IF EXISTS taxi_arrivals\")\n",
    "    cur.execute(\"DROP SOURCE IF EXISTS taxi_arrivals_src\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE MATERIALIZED SOURCE taxi_arrivals_src\n",
    "    FROM KAFKA BROKER 'redpanda:29092' TOPIC 'taxi-arrivals'\n",
    "        KEY FORMAT TEXT\n",
    "        VALUE FORMAT BYTES\n",
    "        INCLUDE KEY AS trip_no, TIMESTAMP AS received_at\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE VIEW taxi_arrivals AS (\n",
    "        SELECT\n",
    "            trip_no,\n",
    "            received_at,\n",
    "            CAST(CONVERT_FROM(data, 'utf8') AS JSONB) AS arrival\n",
    "        FROM taxi_arrivals_src\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "pd.read_sql('SELECT * FROM taxi_arrivals ORDER BY received_at DESC LIMIT 10', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beaver encourages you to process your streaming data with SQL. We'll start by building up some features which we'll then feed to a machine learning model. Let's start simple and calculate two features based on the distance between the pick-up and drop-off locations, as well as some basic temporal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_query = \"\"\"\n",
    "DROP VIEW IF EXISTS taxi_features;\n",
    "\n",
    "CREATE VIEW taxi_features AS (\n",
    "    SELECT \n",
    "        trip_no,\n",
    "        ABS(dropoff_lat - pickup_lat) + ABS(dropoff_lon - pickup_lon) AS manhattan_distance,\n",
    "        SQRT(POWER(dropoff_lat - pickup_lat, 2) + POWER(dropoff_lon - pickup_lon, 2)) AS euclidean_distance,\n",
    "        EXTRACT(HOUR FROM pickup_datetime) AS pickup_hour,\n",
    "        EXTRACT(DOW FROM pickup_datetime) = 1 AS is_monday,\n",
    "        EXTRACT(DOW FROM pickup_datetime) = 2 AS is_tuesday,\n",
    "        EXTRACT(DOW FROM pickup_datetime) = 3 AS is_wednesday,\n",
    "        EXTRACT(DOW FROM pickup_datetime) = 4 AS is_thursday,\n",
    "        EXTRACT(DOW FROM pickup_datetime) = 5 AS is_friday,\n",
    "        EXTRACT(DOW FROM pickup_datetime) = 6 AS is_saturday,\n",
    "        EXTRACT(DOW FROM pickup_datetime) = 7 AS is_sunday\n",
    "    FROM (\n",
    "        SELECT\n",
    "            trip_no,\n",
    "            CAST(trip ->> 'dropoff_latitude' AS FLOAT) AS dropoff_lat,\n",
    "            CAST(trip ->> 'pickup_latitude' AS FLOAT) AS pickup_lat,\n",
    "            CAST(trip ->> 'dropoff_longitude' AS FLOAT) AS dropoff_lon,\n",
    "            CAST(trip ->> 'pickup_longitude' AS FLOAT) AS pickup_lon,\n",
    "            CAST(trip ->> 'pickup_datetime' AS TIMESTAMP) AS pickup_datetime\n",
    "        FROM taxi_departures\n",
    "    )\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of running this query like we did above, we'll register it in Beaver so it can be associated to a model. Registering a feature set happens through the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.post(\n",
    "    \"http://localhost:8000/api/features/\",\n",
    "    json={\n",
    "        \"name\": \"taxi_features\",\n",
    "        \"query\": feature_set_query,\n",
    "        \"key_field\": \"trip_no\",\n",
    "        \"processor_id\": 1\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features we built can be used to do inference. But to train a model, we also need a target. Beaver also encourages you to define this target with SQL. For this example we'll predict the duration in seconds of each trip, which is a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_query = \"\"\"\n",
    "DROP VIEW IF EXISTS taxi_targets;\n",
    "\n",
    "CREATE VIEW taxi_targets AS (\n",
    "    SELECT\n",
    "        trip_no,\n",
    "        CAST(arrival ->> 'duration' AS INTEGER) AS duration\n",
    "    FROM taxi_arrivals\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "requests.post(\n",
    "    \"http://localhost:8000/api/targets/\",\n",
    "    json={\n",
    "        \"name\": \"taxi_targets\",\n",
    "        \"query\": target_query,\n",
    "        \"key_field\": \"trip_no\",\n",
    "        \"target_field\": \"duration\",\n",
    "        \"task\": \"REGRESSION\",\n",
    "        \"processor_id\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending a first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's upload a first model. We'll start with a plain and simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "import dill\n",
    "import requests\n",
    "from river import linear_model, preprocessing\n",
    "\n",
    "model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
    "model.learn = model.learn_one\n",
    "model.predict = model.predict_one\n",
    "\n",
    "requests.post(\n",
    "    \"http://localhost:8000/api/models/\",\n",
    "    json={\n",
    "        \"name\": \"taxis-linear-regression\",\n",
    "        \"task\": \"REGRESSION\",\n",
    "        \"content\": base64.b64encode(dill.dumps(model)).decode(\"ascii\"),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the models are hosted in Beaver, which is why we encode the model and send it in the payload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all we need to run a first experiment. An experiment boils down to training a model on a feature set to predict a target. The model has already been uploaded. Beaver will thus take care of training the model in real-time. Beaver will also issue a prediction for each new arriving sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"id\":2,\"model_state\":\"gASVzgMAAAAAAACMFnJpdmVyLmNvbXBvc2UucGlwZWxpbmWUjAhQaXBlbGluZZSTlCmBlH2UKIwFc3RlcHOUjAtjb2xsZWN0aW9uc5SMC09yZGVyZWREaWN0lJOUKVKUKIwOU3RhbmRhcmRTY2FsZXKUjBlyaXZlci5wcmVwcm9jZXNzaW5nLnNjYWxllIwOU3RhbmRhcmRTY2FsZXKUk5QpgZR9lCiMCHdpdGhfc3RklIiMBmNvdW50c5RoBowHQ291bnRlcpSTlH2UhZRSlIwFbWVhbnOUaAaMC2RlZmF1bHRkaWN0lJOUjApkaWxsLl9kaWxslIwKX2xvYWRfdHlwZZSTlIwFZmxvYXSUhZRSlIWUUpSMBHZhcnOUaBloH4WUUpR1YowQTGluZWFyUmVncmVzc2lvbpSMGnJpdmVyLmxpbmVhcl9tb2RlbC5saW5fcmVnlIwQTGluZWFyUmVncmVzc2lvbpSTlCmBlH2UKIwJb3B0aW1pemVylIwPcml2ZXIub3B0aW0uc2dklIwDU0dElJOUKYGUfZQojAJscpSMFnJpdmVyLm9wdGltLnNjaGVkdWxlcnOUjAhDb25zdGFudJSTlCmBlH2UjA1sZWFybmluZ19yYXRllEc/hHrhR64Ue3NijAxuX2l0ZXJhdGlvbnOUSwB1YowEbG9zc5SMEnJpdmVyLm9wdGltLmxvc3Nlc5SMB1NxdWFyZWSUk5QpgZSMAmwylEcAAAAAAAAAAIwCbDGURwAAAAAAAAAAjA5pbnRlcmNlcHRfaW5pdJRHAAAAAAAAAACMCWludGVyY2VwdJRHAAAAAAAAAACMDGludGVyY2VwdF9scpRoNCmBlH2UaDdHP4R64UeuFHtzYowNY2xpcF9ncmFkaWVudJRHQm0alKIAAACMC2luaXRpYWxpemVylIwYcml2ZXIub3B0aW0uaW5pdGlhbGl6ZXJzlIwFWmVyb3OUk5QpgZR9lIwFdmFsdWWURwAAAAAAAAAAc2KMCF93ZWlnaHRzlIwWcml2ZXIudXRpbHMudmVjdG9yZGljdJSMGV9fcHl4X3VucGlja2xlX1ZlY3RvckRpY3SUk5RoTowKVmVjdG9yRGljdJSTlEp3N2EBToeUUpQofZROiU6JiXSUYowHX3lfbmFtZZROdWJ1jAVsZWFybpRoHIwKTWV0aG9kVHlwZZSFlFKUaACMElBpcGVsaW5lLmxlYXJuX29uZZSTlGgDhpRSlIwHcHJlZGljdJRoW2gAjBRQaXBlbGluZS5wcmVkaWN0X29uZZSTlGgDhpRSlHViLg==\",\"sync_seconds\":20,\"target_id\":2,\"runner_id\":1,\"name\":\"Taxi trips lin reg experiment\",\"n_samples_trained_on\":0,\"feature_set_id\":2,\"model_id\":2,\"sink_id\":1}'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = requests.post(\n",
    "    \"http://localhost:8000/api/experiments/\",\n",
    "    json={\n",
    "        \"name\": \"Taxi trips lin reg experiment\",\n",
    "        \"feature_set_id\": 2,\n",
    "        \"target_id\": 2,\n",
    "        \"model_id\": 2,\n",
    "        \"runner_id\": 1,\n",
    "        \"sink_id\": 1\n",
    "    },\n",
    ")\n",
    "exp.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can monitor progress of the experiment as it keeps running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"now\":\"2023-01-13T00:03:40.844116\",\"training_progress\":0.0,\"mse\":73230.8,\"mae\":246.4}'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor = requests.get(\n",
    "    \"http://localhost:8000/api/experiments/2/monitor\",\n",
    ")\n",
    "\n",
    "monitor.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending a new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: send a random forest on the same dataset. Show how it compares to the existing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: creating stateful features with Materialize. Create a new experiment with the random forest on these features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "55fbbcf542e06cc59ad76a1e0d5dc36ee204d6d2b704491656ee6b3487310122"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
